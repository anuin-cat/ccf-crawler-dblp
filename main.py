import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(ROOT_DIR)

import logging
import time
import asyncio
import argparse

from crawler.fetch_meta import main_papers_meta
from crawler.fetch_abstract import main_papers_abstract
from utils import info_by_dir

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='CCF DBLP çˆ¬è™«ç¨‹åº')
    parser.add_argument('-ccf', type=str, default='b', help='CCF ç­‰çº§ (é»˜è®¤: b)')
    parser.add_argument('-c', '--classification', type=str, default='conf', help='è®ºæ–‡åˆ†ç±»ç±»å‹, å¯é€‰å€¼: conf, journal')
    parser.add_argument('-m', '--max-concurrent', type=int, default=20, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 20)')
    parser.add_argument('-p', '--proxy-pool-size', type=int, default=10, help='ä»£ç†æ± å¤§å° (é»˜è®¤: 10)')
    
    args = parser.parse_args()
    
    classification = args.classification
    ccf = args.ccf

    # 1. å®šä¹‰ä¿å­˜ç›®å½•
    data_dir = os.path.join(ROOT_DIR, 'data', 'paper', f'{classification}_{ccf}')
    log_dir = os.path.join(ROOT_DIR, 'data', 'logs')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    # 2. é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(os.path.join(log_dir, f'log_{int(time.time())}.txt'), mode='w', encoding='utf-8')
        ]
    )
    
    logging.info("=" * 60)
    logging.info("ğŸš€ å¼€å§‹è¿è¡Œ CCF DBLP çˆ¬è™«ç¨‹åº")
    logging.info(f"ğŸ“ æ•°æ®ä¿å­˜ç›®å½•: {data_dir}")
    logging.info(f"ğŸ“ æ—¥å¿—ä¿å­˜ç›®å½•: {log_dir}")
    logging.info(f"ğŸ“‹ åˆ†ç±»: {classification}, CCFç­‰çº§: {ccf}")
    logging.info(f"âš™ï¸ æœ€å¤§å¹¶å‘æ•°: {args.max_concurrent}, ä»£ç†æ± å¤§å°: {args.proxy_pool_size}")
    logging.info("=" * 60)
    
    # 3. è·å–è®ºæ–‡å…ƒä¿¡æ¯
    logging.info("\nğŸ“Š æ­¥éª¤ 1/2: è·å–è®ºæ–‡å…ƒä¿¡æ¯...")
    main_papers_meta(data_dir, ccf=ccf, classification=classification)
    info_by_dir(data_dir)

    # 4. è·å–è®ºæ–‡æ‘˜è¦ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    logging.info("\nğŸ“„ æ­¥éª¤ 2/2: è·å–è®ºæ–‡æ‘˜è¦...")
    asyncio.run(main_papers_abstract(data_dir, max_concurrent=args.max_concurrent, proxy_pool_size=args.proxy_pool_size))
    info_by_dir(data_dir)
