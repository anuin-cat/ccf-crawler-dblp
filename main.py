import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(ROOT_DIR)

import logging
import time
import asyncio

from crawler.fetch_meta import main_papers_meta
from crawler.fetch_abstract import main_papers_abstract
from utils import info_by_dir

if __name__ == "__main__":
    classification = 'conf'
    ccf = 'b'

    # 1. å®šä¹‰ä¿å­˜ç›®å½•
    data_dir = os.path.join(ROOT_DIR, 'data', 'paper', f'{classification}_{ccf}')
    log_dir = os.path.join(ROOT_DIR, 'data', 'logs')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    # 2. é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(os.path.join(log_dir, f'log_{int(time.time())}.txt'), mode='w', encoding='utf-8')
        ]
    )
    
    logging.info("=" * 60)
    logging.info("ğŸš€ å¼€å§‹è¿è¡Œ CCF DBLP çˆ¬è™«ç¨‹åº")
    logging.info(f"ğŸ“ æ•°æ®ä¿å­˜ç›®å½•: {data_dir}")
    logging.info(f"ğŸ“ æ—¥å¿—ä¿å­˜ç›®å½•: {log_dir}")
    logging.info("=" * 60)
    
    # 3. è·å–è®ºæ–‡å…ƒä¿¡æ¯
    # logging.info("\nğŸ“Š æ­¥éª¤ 1/2: è·å–è®ºæ–‡å…ƒä¿¡æ¯...")
    # main_papers_meta(data_dir, ccf=ccf, classification=classification)
    # info_by_dir(data_dir)

    # 4. è·å–è®ºæ–‡æ‘˜è¦ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ - æ¨èï¼‰
    logging.info("\nğŸ“„ æ­¥éª¤ 2/2: è·å–è®ºæ–‡æ‘˜è¦...")
    asyncio.run(main_papers_abstract(data_dir, max_concurrent=20, proxy_pool_size=10))
    info_by_dir(data_dir)