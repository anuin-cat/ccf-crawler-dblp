#!/usr/bin/env python3
"""
DBLP API è®ºæ–‡è·å–æ¨¡å—
é€šè¿‡DBLP APIè·å–æŒ‡å®šä¼šè®®æˆ–æœŸåˆŠçš„è®ºæ–‡ä¿¡æ¯(å¼‚æ­¥ç‰ˆæœ¬)

AsyncAbstractFetcher: å¼‚æ­¥æ‘˜è¦è·å–å™¨
- æ”¯æŒé«˜å¹¶å‘å¼‚æ­¥è¯·æ±‚
- æ›´é«˜æ•ˆçš„I/Oå¤„ç†
- æ”¯æŒå¤šç§æ–¹å¼è·å–æ‘˜è¦
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(ROOT_DIR)

import json
import logging
import time
import asyncio
import aiohttp
import ssl

from typing import List, Optional, Tuple
from pathlib import Path
from tqdm import tqdm
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from typing import Literal

from driver import PlaywrightDriver, ProxyPool
from crawler.fetch_meta import DBLPMetaFetcher

# å¯¼å…¥CCF Aç±»ä¼šè®®è§„åˆ™
from utils import suppress_all_output, info_by_dir
from config.venue import get_all_venue_by_rule


class AsyncAbstractFetcher:
    """
    å¼‚æ­¥è®ºæ–‡æ‘˜è¦è·å–å™¨
    
    ä½¿ç”¨å¼‚æ­¥I/Oå¤„ç†ï¼Œæ”¯æŒé«˜å¹¶å‘è¯·æ±‚ï¼Œæ˜¾è‘—æé«˜å¤„ç†æ•ˆç‡
    """
    
    def __init__(self, max_concurrent: int = 10, proxy_pool_size: int = 10):
        """ 
        åˆå§‹åŒ–å¼‚æ­¥æ‘˜è¦è·å–å™¨
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
        """
        self.crossref_base_url = "https://api.crossref.org/works/"
        self.openalex_base_url = "https://api.openalex.org/works/"
        self.semantic_scholar_base_url = "https://api.semanticscholar.org/v1/paper/"
        self.ua = UserAgent()
        self.driver = PlaywrightDriver(max_concurrent=10, proxy_pool_size=10, headless=True, timeout=120000)

        # çº¿ç¨‹æ± 
        self.proxy_pool = ProxyPool(pool_size=proxy_pool_size)

        # å¼‚æ­¥é…ç½®
        self.max_concurrent = max_concurrent
        self.semaphore = None
        self.session = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats_map = {}
        self.stats_keys = [
            'total_papers',
            'papers_with_abstract',
            'papers_without_doi',
            'papers_without_doi_and_url',
            'papers_abstract_fetched',
            'papers_abstract_failed'
        ]

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œç¦ç”¨è¯ä¹¦éªŒè¯ä»¥è§£å†³arxiv.orgç­‰ç½‘ç«™çš„SSLé—®é¢˜
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # åˆ›å»ºTCPè¿æ¥å™¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰SSLä¸Šä¸‹æ–‡
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            headers={
                'User-Agent': self.ua.random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            },
            timeout=aiohttp.ClientTimeout(total=12)
        )
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def _clean_abstract(self, abstract: str) -> str:
        """
        æ¸…ç†æ‘˜è¦æ–‡æœ¬ï¼Œç§»é™¤HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºç™½å­—ç¬¦
        
        Args:
            abstract: åŸå§‹æ‘˜è¦æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ‘˜è¦æ–‡æœ¬
        """
        import re
        # ç§»é™¤HTMLæ ‡ç­¾
        abstract = re.sub(r'<[^>]+>', '', abstract)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        abstract = re.sub(r'\s+', ' ', abstract).strip()
        return abstract
    
    async def _request_with_retry_async(self, url: str, doi: str, api_name: str = "API", retry_delays: List[float] = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 3, 10]) -> Optional[dict]:
        """
        å¼‚æ­¥å¸¦é‡è¯•æœºåˆ¶çš„APIè¯·æ±‚
        
        Args:
            url: APIè¯·æ±‚URL
            doi: è®ºæ–‡DOIï¼ˆç”¨äºæ—¥å¿—ï¼‰
            api_name: APIåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            retry_delays: é‡è¯•å»¶è¿Ÿæ—¶é—´åˆ—è¡¨
            
        Returns:
            APIå“åº”çš„JSONæ•°æ®ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        # é‡è¯•é…ç½®ï¼šå»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        max_retries = len(retry_delays)
        
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            for attempt in range(max_retries + 1):
                try:
                    async with self.session.get(
                        url, 
                        proxy=self.proxy_pool.get_proxy_url(),
                        timeout=aiohttp.ClientTimeout(total=3.6)
                    ) as response:
                        if response.status == 404:
                            return None
                        response.raise_for_status()
                        return await response.json()
                        
                except Exception as e:
                    if attempt < max_retries:
                        delay = retry_delays[attempt]
                        # logging.warning(f"{api_name}è·å–æ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯ {doi} (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                        await asyncio.sleep(delay)
                    else:
                        logging.error(f"{api_name}è·å–æ‘˜è¦æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {doi} - {e}")
                        return None
        
        return None
    
    async def _request_html_with_retry_async(self, url: str, source_name: str = "ç½‘é¡µ") -> Optional[str]:
        """
        å¼‚æ­¥å¸¦é‡è¯•æœºåˆ¶çš„HTMLé¡µé¢è¯·æ±‚
        
        Args:
            url: ç½‘é¡µURL
            source_name: æ¥æºåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            ç½‘é¡µçš„HTMLå†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        # é‡è¯•é…ç½®ï¼šå»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        retry_delays = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
        max_retries = len(retry_delays)
        
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            for attempt in range(max_retries + 1):
                try:
                    async with self.session.get(url, proxy=self.proxy_pool.get_proxy_url(with_auth=True)) as response:
                        if response.status == 404:
                            return None
                        response.raise_for_status()
                        return await response.text()
                        
                except Exception as e:
                    if attempt < max_retries:
                        delay = retry_delays[attempt]
                        if delay > 1:
                            print(f"âš ï¸ {source_name} è·å–é¡µé¢å¤±è´¥ï¼Œç­‰å¾… {delay} ç§’åé‡è¯•: {url}, é”™è¯¯ä¿¡æ¯: {e}")
                        await asyncio.sleep(delay)
                    else:
                        logging.error(f"{source_name}è·å–é¡µé¢æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {url} - {e}")
                        return None
        
        return None

    # ========================== å¤šæ–¹å¼é€šè¿‡ url è·å–æ‘˜è¦ ==========================

    async def fetch_abstract_from_acm(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä»ACM é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        with suppress_all_output():
            html = await self.driver.safe_get(url, ['#abstract', '.abstractSection'], max_retries=5)
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. æŸ¥æ‰¾æ‘˜è¦section - ACMé¡µé¢é€šå¸¸æœ‰idä¸º"abstract"çš„section
        abstract_section = soup.find('section', id='abstract')
        if abstract_section:
            # æŸ¥æ‰¾æ‰€æœ‰æ®µè½å…ƒç´ 
            paragraphs = abstract_section.find_all('div', role='paragraph')
            if paragraphs:
                # å°†æ‰€æœ‰æ®µè½æ–‡æœ¬åˆå¹¶
                abstract_text = ' '.join([p.get_text(strip=True) for p in paragraphs])
                return abstract_text

        # 2. æŸ¥æ‰¾æ‘˜è¦ div - class ä¸º abstractSection
        abstract_div = soup.find('div', class_='abstractSection')
        if abstract_div:
            # æŸ¥æ‰¾æ‰€æœ‰æ®µè½å…ƒç´ 
            paragraphs = abstract_div.find_all('p')
            if paragraphs:
                # å°†æ‰€æœ‰æ®µè½æ–‡æœ¬åˆå¹¶
                abstract_text = ' '.join([p.get_text(strip=True) for p in paragraphs])
                return abstract_text

        return None

    async def fetch_abstract_from_acl(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» ACL Anthology é¡µé¢è·å–è®ºæ–‡æ‘˜è¦

        éƒ¨åˆ†æ–‡ç« ä¸åŒ…å«æ‘˜è¦
        """

        def fetch_abstract_from_acl_html(html: str) -> str:
            if html is None: return None

            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - é€šå¸¸åŒ…å«åœ¨classä¸º"acl-abstract"çš„divä¸­
            abstract_div = soup.find('div', class_='acl-abstract')
            if abstract_div:
                # æŸ¥æ‰¾æ‘˜è¦å†…å®¹ - é€šå¸¸åœ¨spanæ ‡ç­¾ä¸­
                abstract_span = abstract_div.find('span')
                if abstract_span:
                    # è·å–æ‘˜è¦æ–‡æœ¬å¹¶æ¸…ç†
                    abstract_text = abstract_span.get_text(strip=True)
                    return abstract_text
            return None

        # ================ ä½¿ç”¨ request è·å– ================
        html = await self._request_html_with_retry_async(url, "ACL Anthology")
        abstract_text = fetch_abstract_from_acl_html(html)
        if abstract_text:
            return abstract_text

        # ================ ä½¿ç”¨ playwright è·å– ================
        with suppress_all_output():
            html = await self.driver.safe_get(url, [], max_retries=5)
        abstract_text = fetch_abstract_from_acl_html(html)
        if abstract_text:
            return abstract_text

        return None

    async def fetch_abstract_from_openaccess(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» openaccess é¡µé¢è·å–è®ºæ–‡æ‘˜è¦ (cvpr)
        """
        html = await self._request_html_with_retry_async(url, "OpenAccess")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - é€šå¸¸åŒ…å« id="abstract" çš„ div å†…çš„ p æ ‡ç­¾
        abstract_div = soup.find('div', id='abstract')
        if abstract_div:
            abstract_text = abstract_div.get_text(strip=True)
            return abstract_text
        return None

    async def fetch_abstract_from_usenix(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» usenix é¡µé¢è·å–è®ºæ–‡æ‘˜è¦ (FAST, NSDI, OSDI)
        """
        html = await self._request_html_with_retry_async(url, "OpenAccess")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - å®Œæ•´çš„classåŒ¹é…
        abstract_div = soup.find('div', class_='field field-name-field-paper-description field-type-text-long field-label-above')
        if abstract_div:
            # æŸ¥æ‰¾ field-items å®¹å™¨
            field_items = abstract_div.find('div', class_='field-items')
            if field_items:
                # æŸ¥æ‰¾ field-item å†…å®¹
                field_item = field_items.find('div', class_='field-item')
                if field_item:
                    # æå–æ‰€æœ‰pæ ‡ç­¾çš„æ–‡æœ¬å†…å®¹
                    paragraphs = field_item.find_all('p')
                    if paragraphs:
                        # åˆå¹¶æ‰€æœ‰æ®µè½ï¼Œç”¨æ¢è¡Œç¬¦åˆ†éš”
                        abstract_parts = []
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            if text:  # è¿‡æ»¤ç©ºæ®µè½
                                abstract_parts.append(text)
                        if abstract_parts:
                            return ' '.join(abstract_parts)
                    
                    # å¦‚æœæ²¡æœ‰pæ ‡ç­¾ï¼Œç›´æ¥è·å–æ–‡æœ¬
                    abstract_text = field_item.get_text(strip=True)
                    # æ¸…ç†æ‰å¯èƒ½çš„æ ‡ç­¾æ®‹ç•™
                    if abstract_text and not abstract_text.startswith('Abstract:'):
                        return abstract_text
        
        return None

    async def fetch_abstract_from_openreview(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» OpenReview é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        html = await self._request_html_with_retry_async(url, "OpenReview")
        if html is None:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - OpenReviewçš„æ‘˜è¦åœ¨note-contentä¸­
        # å¯»æ‰¾åŒ…å«"Abstract"çš„strongæ ‡ç­¾ï¼Œæ–‡æœ¬å¯èƒ½è¢«åˆ†å‰²æˆå¤šä¸ªèŠ‚ç‚¹
        abstract_strongs = soup.find_all('strong', class_='note-content-field')
        for strong in abstract_strongs:
            # è·å–å®Œæ•´çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…æ‹¬åˆ†æ®µçš„æ–‡æœ¬èŠ‚ç‚¹
            full_text = strong.get_text(strip=True)
            if 'Abstract' in full_text and ':' in full_text:
                # åœ¨åŒä¸€ä¸ªçˆ¶å®¹å™¨ä¸­æŸ¥æ‰¾note-content-value
                parent = strong.parent
                if parent:
                    # æŸ¥æ‰¾markdownæ¸²æŸ“çš„å†…å®¹
                    markdown_content = parent.find(class_='note-content-value')
                    if markdown_content:
                        # æå–æ‰€æœ‰pæ ‡ç­¾çš„æ–‡æœ¬å†…å®¹
                        paragraphs = markdown_content.find_all('p')
                        if paragraphs:
                            abstract_parts = []
                            for p in paragraphs:
                                text = p.get_text(strip=True)
                                if text:
                                    abstract_parts.append(text)
                            if abstract_parts:
                                return ' '.join(abstract_parts)
                        
                        # å¦‚æœæ²¡æœ‰pæ ‡ç­¾ï¼Œç›´æ¥è·å–æ–‡æœ¬
                        abstract_text = markdown_content.get_text(strip=True)
                        if abstract_text:
                            return abstract_text

        return None

    async def fetch_abstract_from_mlr(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» Proceedings of Machine Learning Research é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        html = await self._request_html_with_retry_async(url, "MLR")
        if html is None:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - MLRçš„æ‘˜è¦åœ¨ id=abstract çš„ div ä¸­
        abstract_div = soup.find('div', id='abstract')
        if abstract_div:
            abstract_text = abstract_div.get_text(strip=True)
            if abstract_text:
                return abstract_text

        return None

    async def fetch_abstract_from_ijcai(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» ijcai é¡µé¢è·å–è®ºæ–‡æ‘˜è¦ (ijcai)

        æœ‰äº›æ–‡ç« çš„é¡µé¢ç»“æ„ä¸å¤ªä¸€æ ·
        """
        html = await self._request_html_with_retry_async(url, "IJCAI")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. åŒ…å« ijcai.org/proceedings/xxx
        proceedings_detail = soup.find('div', class_='container-fluid proceedings-detail')
        if proceedings_detail:
            # æŸ¥æ‰¾æ‰€æœ‰çš„row
            rows = proceedings_detail.find_all('div', class_='row')
            if len(rows) >= 3:  # ç¬¬ä¸‰ä¸ªrowåŒ…å«æ‘˜è¦å†…å®¹
                abstract_row = rows[2]  # ç´¢å¼•ä»0å¼€å§‹ï¼Œç¬¬ä¸‰ä¸ªrow
                # æŸ¥æ‰¾col-md-12å®¹å™¨
                col_divs = abstract_row.find_all('div', class_='col-md-12')
                if col_divs:
                    # ç¬¬ä¸€ä¸ªcol-md-12é€šå¸¸åŒ…å«æ‘˜è¦æ–‡æœ¬
                    abstract_div = col_divs[0]
                    abstract_text = abstract_div.get_text(strip=True)
                    
                    # è¿‡æ»¤æ‰Keywordséƒ¨åˆ†
                    if 'Keywords:' in abstract_text:
                        abstract_text = abstract_text.split('Keywords:')[0].strip()
                    
                    if abstract_text:
                        return abstract_text
        
        # 2. åŒ…å« ijcai.org/Abstract/xxx
        content_detail = soup.find('div', class_='region region-content')
        if content_detail:
            # æŸ¥æ‰¾ class=content çš„ div å®¹å™¨
            content_div = content_detail.find('div', class_='content')
            if content_div:
                # æŸ¥æ‰¾ç¬¬äºŒä¸ª p æ ‡ç­¾
                p_tags = content_div.find_all('p')
                if len(p_tags) >= 2:
                    abstract_text = p_tags[1].get_text(strip=True)
                    if abstract_text:
                        return abstract_text
        
        return None

    async def fetch_abstract_from_ndss(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» ndss é¡µé¢è·å–è®ºæ–‡æ‘˜è¦ (ndss)
        """
        html = await self._request_html_with_retry_async(url, "NDSS")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. åŒ…å« ndss-paper/xxx
        entry_content = soup.find('div', class_='entry-content')
        if entry_content:
            # æŸ¥æ‰¾paper-dataå®¹å™¨
            paper_data = entry_content.find('div', class_='paper-data')
            if paper_data:
                # åªæ‰¾åˆ°ç›´æ¥å­çº§çš„pæ ‡ç­¾ï¼Œé¿å…åµŒå¥—pæ ‡ç­¾å¯¼è‡´çš„é‡å¤
                p_tags = paper_data.find_all('p', recursive=False)
                
                abstract_parts = []
                found_author_section = False
                
                for p in p_tags:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«strongæ ‡ç­¾ï¼ˆä½œè€…ä¿¡æ¯ï¼‰
                    if p.find('strong'):
                        found_author_section = True
                        continue
                    
                    # å¦‚æœå·²ç»æ‰¾åˆ°ä½œè€…éƒ¨åˆ†ï¼Œå¼€å§‹æ”¶é›†æ‘˜è¦æ–‡æœ¬
                    if found_author_section:
                        # å¯¹äºåµŒå¥—çš„pæ ‡ç­¾ï¼Œè·å–å†…éƒ¨æ‰€æœ‰pæ ‡ç­¾çš„æ–‡æœ¬
                        inner_p_tags = p.find_all('p')
                        if inner_p_tags:
                            # å¦‚æœæœ‰å†…éƒ¨pæ ‡ç­¾ï¼Œæå–å®ƒä»¬çš„æ–‡æœ¬
                            for inner_p in inner_p_tags:
                                text = inner_p.get_text(strip=True)
                                if text:
                                    abstract_parts.append(text)
                        else:
                            # å¦‚æœæ²¡æœ‰å†…éƒ¨pæ ‡ç­¾ï¼Œç›´æ¥è·å–æ–‡æœ¬
                            text = p.get_text(strip=True)
                            if text:
                                abstract_parts.append(text)
                
                if abstract_parts:
                    # åˆå¹¶æ‰€æœ‰æ‘˜è¦æ®µè½ï¼Œå¹¶å»é‡
                    unique_parts = []
                    seen = set()
                    for part in abstract_parts:
                        if part not in seen:
                            seen.add(part)
                            unique_parts.append(part)

                    if unique_parts:
                        abstract_text = ' '.join(unique_parts)
                        return abstract_text
        
        # 2. å…¶ä»– - å¤„ç†åŒ…å« "Abstract:" çš„æ–°æ ¼å¼é¡µé¢
        section_content = soup.find('section', class_='new-wrapper')
        if section_content:
            # æŸ¥æ‰¾åŒ…å« "Abstract:" çš„ h2 æ ‡ç­¾
            abstract_h2 = section_content.find('h2', string=lambda text: text and 'Abstract:' in text)
            if abstract_h2:
                # æ”¶é›† h2 æ ‡ç­¾åé¢çš„æ‰€æœ‰ p æ ‡ç­¾ä¸­çš„æ–‡æœ¬
                abstract_parts = []
                next_element = abstract_h2.find_next_sibling() # ä¸ä¼šè¶Šç•Œï¼ŒåªæŸ¥æ‰¾ä¸å½“å‰å…ƒç´ åŒçº§çš„ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
                
                while next_element:
                    if next_element.name == 'p':
                        # è·å– p æ ‡ç­¾çš„æ–‡æœ¬å†…å®¹
                        text = next_element.get_text(strip=True)
                        if text:
                            abstract_parts.append(text)
                    next_element = next_element.find_next_sibling()
                
                if abstract_parts:
                    # ä½¿ç”¨ç©ºæ ¼è¿æ¥æ‰€æœ‰æ‘˜è¦æ®µè½
                    abstract_text = ' '.join(abstract_parts)
                    return abstract_text

        return None
    
    async def fetch_abstract_from_nips(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» NIPS proceedings é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        html = await self._request_html_with_retry_async(url, "NIPS")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - æŸ¥æ‰¾h4æ ‡é¢˜ä¸º"Abstract"çš„å…ƒç´ 
        abstract_h4 = soup.find('h4', string='Abstract')
        if abstract_h4:
            # æŸ¥æ‰¾ç´§è·Ÿåœ¨Abstractæ ‡é¢˜åçš„pæ ‡ç­¾
            next_element = abstract_h4.find_next_sibling()
            while next_element:
                if next_element.name == 'p':
                    # æå–pæ ‡ç­¾å†…çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹
                    abstract_text = next_element.get_text(strip=True)
                    if abstract_text:
                        return abstract_text
                    break
                next_element = next_element.find_next_sibling()
        
        return None

    async def fetch_abstract_from_arxiv(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» arxiv é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        html = await self._request_html_with_retry_async(url, "arXiv")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - æŸ¥æ‰¾classä¸º"abstract mathjax"çš„blockquoteå…ƒç´ 
        abstract_blockquote = soup.find('blockquote', class_='abstract mathjax')
        if abstract_blockquote:
            # æŸ¥æ‰¾Abstract:æ ‡ç­¾
            descriptor_span = abstract_blockquote.find('span', class_='descriptor')
            if descriptor_span and 'Abstract:' in descriptor_span.get_text():
                # è·å–blockquoteå†…çš„æ‰€æœ‰æ–‡æœ¬ï¼Œä½†æ’é™¤descriptor spançš„æ–‡æœ¬
                abstract_text = abstract_blockquote.get_text(strip=True)
                # ç§»é™¤"Abstract:"å‰ç¼€
                if abstract_text.startswith('Abstract:'):
                    abstract_text = abstract_text[9:].strip()
                return abstract_text
        
        return None

    async def fetch_abstract_from_springer(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» springer é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        html = await self._request_html_with_retry_async(url, "Springer")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - æŸ¥æ‰¾ id ä¸º Abs1-content çš„ div å…ƒç´ ï¼Œå…¶å†…æ‰€æœ‰ p çš„ text å³ä¸ºæ‘˜è¦
        abstract_div = soup.find('div', id='Abs1-content')
        if abstract_div:
            # è·å–æ‰€æœ‰ p æ ‡ç­¾çš„æ–‡æœ¬
            abstract_text = ' '.join([p.get_text(strip=True) for p in abstract_div.find_all('p')])
            return abstract_text
        
        return None

    async def fetch_abstract_from_ieee(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» ieee é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        with suppress_all_output():
            # html = await self.driver.safe_get(url, ['.u-mb-1'], max_retries=5)
            html = await self.driver.safe_get(url, [], max_retries=5)
        # html = await self._request_html_with_retry_async(url, "IEEE")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # æ–¹æ³•1: å…ˆæŸ¥æ‰¾å…·æœ‰u-mb-1 classçš„çˆ¶å®¹å™¨ (åªåŒ…å« u-mb-1)ï¼Œç¡®ä¿æ‰¾åˆ°æ­£ç¡®çš„æ‘˜è¦åŒºåŸŸ
        abstract_containers = soup.find_all('div', class_='u-mb-1')

        # æ‰¾åˆ° abstract_container ä¸­åŒ…å« abstract çš„å®¹å™¨
        abstract_container = None
        for abstract_container in abstract_containers:
            if 'abstract' in abstract_container.get_text(strip=True).lower():
                break # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å« abstract çš„å®¹å™¨
        
        if abstract_container:
            # åœ¨è¯¥å®¹å™¨å†…æŸ¥æ‰¾åŒ…å« "Abstract" çš„ h2 æ ‡ç­¾
            abstract_h2 = abstract_container.find('h2', string=lambda text: text and 'Abstract' in text.strip())
            if abstract_h2:
                # åœ¨åŒä¸€å®¹å™¨å†…æŸ¥æ‰¾å¸¦æœ‰xplmathjaxå±æ€§çš„div
                abstract_div = abstract_container.find('div', attrs={'xplmathjax': True})
                if abstract_div:
                    abstract_text = abstract_div.get_text(strip=True)
                    if abstract_text:
                        return abstract_text
                
                # å¦‚æœæ²¡æ‰¾åˆ°xplmathjaxå±æ€§çš„divï¼ŒæŸ¥æ‰¾h2åé¢ç´§è·Ÿçš„div
                next_element = abstract_h2.find_next_sibling()
                while next_element:
                    if next_element.name == 'div':
                        abstract_text = next_element.get_text(strip=True)
                        if abstract_text:
                            return abstract_text
                        break
                    next_element = next_element.find_next_sibling()
        
        return None

    async def fetch_abstract_from_aaai(self, url: str) -> Optional[str]:
        """
        å¼‚æ­¥ä» aaai é¡µé¢è·å–è®ºæ–‡æ‘˜è¦
        """
        with suppress_all_output():
            html = await self.driver.safe_get(url, [], max_retries=5)
        # html = await self._request_html_with_retry_async(url, "AAAI")
        if html is None:
            return None
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. æŸ¥æ‰¾æ‘˜è¦å…ƒç´  - æŸ¥æ‰¾ class = 'item abstract' çš„ section å…ƒç´ 
        abstract_section = soup.find('section', class_='item abstract')
        if abstract_section:
            # å…ˆç§»é™¤ section å†…çš„ h2 æ ‡ç­¾
            h2_tag = abstract_section.find('h2')
            if h2_tag: h2_tag.decompose()

            # è·å–å‰©ä½™å†…å®¹ä½œä¸ºæ‘˜è¦
            abstract_text = abstract_section.get_text(strip=True)
            if abstract_text:
                return abstract_text
        
        # 2. ç¬¬äºŒç§ç½‘é¡µå½¢å¼ - æŸ¥æ‰¾ paper-section-wrap ç»“æ„
        # æŸ¥æ‰¾åŒ…å« "Abstract:" çš„ h4 æ ‡ç­¾çš„çˆ¶å®¹å™¨
        abstract_containers = soup.find_all('div', class_='paper-section-wrap')
        for container in abstract_containers:
            h4_tag = container.find('h4')
            if h4_tag and 'Abstract:' in h4_tag.get_text(strip=True):
                # åœ¨è¯¥å®¹å™¨å†…æŸ¥æ‰¾ attribute-output çš„ div
                attribute_output = container.find('div', class_='attribute-output')
                if attribute_output:
                    # è·å– p æ ‡ç­¾ä¸­çš„æ–‡æœ¬å†…å®¹
                    p_tag = attribute_output.find('p')
                    if p_tag:
                        abstract_text = p_tag.get_text(strip=True)
                        if abstract_text:
                            return abstract_text

        return None

    async def fetch_abstract_by_url_async(self, url: str, venue_name: str = None) -> Optional[str]:
        """
        å¼‚æ­¥ä½¿ç”¨å¤šä¸ªAPIæºè·å–æ‘˜è¦

        Args:
            url: è®ºæ–‡URL
            
        Returns:
            è®ºæ–‡æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœæ‰€æœ‰APIéƒ½è·å–å¤±è´¥è¿”å›None
        """

        if 'pdf' in url:
            return None

        if 'aclanthology' in url or 'findings-acl' in url or 'acl' in url:
            return await self.fetch_abstract_from_acl(url)
        elif 'dl.acm.org' in url:
            return await self.fetch_abstract_from_acm(url)
        elif 'openaccess' in url:
            return await self.fetch_abstract_from_openaccess(url)
        elif 'ijcai' in url:
            return await self.fetch_abstract_from_ijcai(url)
        elif 'usenix' in url:
            return await self.fetch_abstract_from_usenix(url)
        elif 'ndss' in url:
            return await self.fetch_abstract_from_ndss(url)
        elif 'nips' in url or 'neurips' in url:
            return await self.fetch_abstract_from_nips(url)
        elif 'arxiv' in url:
            return await self.fetch_abstract_from_arxiv(url)
        elif 'openreview' in url:
            return await self.fetch_abstract_from_openreview(url)
        elif 'proceedings.mlr' in url:
            return await self.fetch_abstract_from_mlr(url)
        elif 'springer' in url:
            return await self.fetch_abstract_from_springer(url)
        elif 'ieee' in url:
            return await self.fetch_abstract_from_ieee(url)
        elif 'aaai' in url:
            return await self.fetch_abstract_from_aaai(url)
        
        # ç‰¹æ®Šæƒ…å†µ
        elif 'doi.org' in url and venue_name and venue_name in ['crypto', 'eurocrypt', 'fm', 'cav', 'wine', 'eccv']:
            return await self.fetch_abstract_from_springer(url)
        elif 'doi.org' in url and venue_name and venue_name in ['mm', 'icmr']:
            return await self.fetch_abstract_from_acm(url)
        elif 'doi.org' in url and venue_name and venue_name in ['emnlp', 'naacl', 'acl']:
            return await self.fetch_abstract_from_acl(url)
        elif 'doi.org' in url and venue_name and venue_name in ['icaps']:
            return await self.fetch_abstract_from_aaai(url)
        elif 'doi.org' in url and venue_name and venue_name in ['icassp', 'icme']:
            return await self.fetch_abstract_from_ieee(url)

        # å…¶ä»–æƒ…å†µ
        else:
            return None

    # ========================== å¤šæ–¹å¼é€šè¿‡ doi è·å–æ‘˜è¦ ==========================

    async def fetch_abstract_from_crossref(self, doi: str) -> Optional[str]:
        """
        å¼‚æ­¥ä»CrossRef APIè·å–è®ºæ–‡æ‘˜è¦
        
        Args:
            doi: è®ºæ–‡DOI
            
        Returns:
            è®ºæ–‡æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        url = f"{self.crossref_base_url}{doi}"
        data = await self._request_with_retry_async(url, doi, "CrossRef", retry_delays=[0.1, 0.1])
        
        if data is None:
            return None
        
        message = data.get('message', {})
        abstract = message.get('abstract', '')
        
        if abstract:
            return self._clean_abstract(abstract)
        
        return None
    
    async def fetch_abstract_from_openalex(self, doi: str) -> Optional[str]:
        """
        å¼‚æ­¥ä»OpenAlex APIè·å–è®ºæ–‡æ‘˜è¦
        
        Args:
            doi: è®ºæ–‡DOI
            
        Returns:
            è®ºæ–‡æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        url = f"{self.openalex_base_url}doi:{doi}"
        data = await self._request_with_retry_async(url, doi, "OpenAlex", retry_delays=[0.1, 0.1])
        
        if data is None:
            return None
        
        # OpenAlexè¿”å›çš„æ‘˜è¦å¯èƒ½åœ¨abstract_inverted_indexå­—æ®µä¸­
        abstract_inverted_index = data.get('abstract_inverted_index', {})
        
        if abstract_inverted_index:
            # å°†å€’æ’ç´¢å¼•è½¬æ¢ä¸ºå®Œæ•´æ–‡æœ¬
            word_positions = []
            for word, positions in abstract_inverted_index.items():
                for pos in positions:
                    word_positions.append((pos, word))
            
            # æŒ‰ä½ç½®æ’åº
            word_positions.sort(key=lambda x: x[0])
            
            # é‡å»ºæ‘˜è¦æ–‡æœ¬
            abstract = ' '.join([word for _, word in word_positions])
            
            if abstract:
                return self._clean_abstract(abstract)
        
        return None
    
    async def fetch_abstract_from_semantic_scholar(self, doi: str) -> Optional[str]:
        """
        å¼‚æ­¥ä»Semantic Scholar APIè·å–è®ºæ–‡æ‘˜è¦
        
        Args:
            doi: è®ºæ–‡DOI
            
        Returns:
            è®ºæ–‡æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        """
        url = f"{self.semantic_scholar_base_url}{doi}"
        data = await self._request_with_retry_async(url, doi, "Semantic Scholar", retry_delays=[0.1, 0.1])
        
        if data is None:
            return None
        
        # Semantic Scholarè¿”å›çš„æ‘˜è¦åœ¨abstractå­—æ®µä¸­
        abstract = data.get('abstract', '')
        
        if abstract:
            return self._clean_abstract(abstract)
        
        return None
    
    async def fetch_abstract_by_doi_async(self, doi: str) -> Optional[str]:
        """
        å¼‚æ­¥ä½¿ç”¨å¤šä¸ªAPIæºè·å–æ‘˜è¦ï¼Œå…·æœ‰å›é€€æœºåˆ¶
        
        Args:
            doi: è®ºæ–‡DOI
            
        Returns:
            è®ºæ–‡æ‘˜è¦æ–‡æœ¬ï¼Œå¦‚æœæ‰€æœ‰APIéƒ½è·å–å¤±è´¥è¿”å›None
        """
        # 1. å°è¯•OpenAlex API
        abstract = await self.fetch_abstract_from_openalex(doi)
        if abstract:
            return abstract

        # 2. å°è¯•Semantic Scholar API
        # abstract = await self.fetch_abstract_from_semantic_scholar(doi)
        # if abstract:
        #     return abstract

        # 3. å°è¯•CrossRef API
        abstract = await self.fetch_abstract_from_crossref(doi)
        if abstract:
            return abstract
        
        return None
    
    # ========================== å¤šçº§åˆ«å¤„ç†è®ºæ–‡ ==========================

    async def process_paper_async(self, paper: dict, json_file: Path) -> bool:
        """
        å¼‚æ­¥å¤„ç†å•ç¯‡è®ºæ–‡çš„æ‘˜è¦è·å–
        
        Args:
            paper: è®ºæ–‡æ•°æ®å­—å…¸
            json_file: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ‘˜è¦
        """
        # 1. å¤„ç†æ–‡ä»¶ä¿¡æ¯
        if json_file not in self.stats_map:
            self.stats_map[json_file] = {key: 0 for key in self.stats_keys}

        self.stats_map[json_file]['total_papers'] += 1
        venue_name = json_file.name.split('_')[0]
        
        # 2. ä¿¡æ¯æ£€æŸ¥
        # æ£€æŸ¥æ˜¯å¦æœ‰DOI
        doi = paper.get('doi', '')
        url = paper.get('ee', [''])[0]  # å‡è®¾eeæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 

        if not doi and not url:
            self.stats_map[json_file]['papers_without_doi_and_url'] += 1
            self.stats_map[json_file]['papers_without_doi'] += 1
            return False
        
        if not doi:
            self.stats_map[json_file]['papers_without_doi'] += 1

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ‘˜è¦
        if 'abstract' in paper and paper['abstract']:
            self.stats_map[json_file]['papers_with_abstract'] += 1
            return False
        
        # 3. é€šè¿‡å¤šä¸ªAPIæºè·å–æ‘˜è¦ï¼ˆå…·æœ‰å›é€€æœºåˆ¶ï¼‰
        abstract = None
        # must_url_venues = ['eccv', 'emnlp'] # eccv çš„ doi æ£€ç´¢ä¸åˆ°
        must_url_venues = ['icme', 'icassp'] # ä¸´æ—¶ç”¨ï¼Œæœ‰äº›ä¼šè®®åœ¨æœ€æ–°çš„å¹´ä»½ä¸­ doi æ²¡åŠæ³•æ‹¿åˆ°æ•°æ®
        if doi and venue_name not in must_url_venues:
            abstract = await self.fetch_abstract_by_doi_async(doi)
        if url and not abstract:
            abstract = await self.fetch_abstract_by_url_async(url, venue_name)
        if not abstract:
            self.stats_map[json_file]['papers_abstract_failed'] += 1
            return False

        paper['abstract'] = abstract
        self.stats_map[json_file]['papers_abstract_fetched'] += 1
        return True
    
    async def process_papers_async(self, papers: List[dict], json_file: Path) -> Tuple[int, int]:
        """
        å¼‚æ­¥å¹¶å‘å¤„ç†ä¸€æ‰¹è®ºæ–‡çš„æ‘˜è¦è·å–
        
        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
            json_file: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            (æˆåŠŸå¤„ç†çš„è®ºæ–‡æ•°, æ€»è®ºæ–‡æ•°)
        """
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        if json_file not in self.stats_map:
            self.stats_map[json_file] = {key: 0 for key in self.stats_keys}
        self.stats_map[json_file]['total_papers'] += len(papers)

        # è¿‡æ»¤å¾—åˆ°æ²¡æœ‰æ‘˜è¦çš„è®ºæ–‡
        filted_papers = [paper for paper in papers if ('abstract' not in paper and paper.get('type', '') != 'Editorship')]
        if len(filted_papers) == 0:
            return 0, len(filted_papers)
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰è®ºæ–‡ï¼Œè¿™æ ·ç”¨çš„å‰ææ˜¯åº•å±‚ä¸€å®šå…±ç”¨ semaphore æ¥é™åˆ¶å¹¶å‘æ•°é‡
        tasks = [asyncio.create_task(self.process_paper_async(paper, json_file)) for paper in filted_papers]
        pbar = tqdm(total=len(filted_papers), desc=f"Processing {json_file.name}", unit=" paper", leave=False)

        successful = 0
        for coro in asyncio.as_completed(tasks):
            res = await coro
            if res:
                successful += 1
            pbar.update(1)
        pbar.close()
        return successful, len(filted_papers)
        
    async def process_file_async(self, json_file: Path) -> bool:
        """
        å¼‚æ­¥å¤„ç†å•ä¸ªJSONæ–‡ä»¶
        
        Args:
            json_file: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶æ˜¯å¦è¢«æ›´æ–°
        """
        try:
            # è¯»å–JSONæ–‡ä»¶
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            papers = data.get('papers', [])
            
            # ä½¿ç”¨å¼‚æ­¥å¤„ç†
            successful_count, total_count = await self.process_papers_async(papers, json_file)
            file_updated = successful_count > 0
            
            logging.info(f"ğŸ“Š æ–‡ä»¶ {json_file.name}: æˆåŠŸå¤„ç† {successful_count}/{total_count} ç¯‡è®ºæ–‡")

            # å¦‚æœæ–‡ä»¶æœ‰æ›´æ–°ï¼Œä¿å­˜å›å»
            if file_updated:
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            return file_updated
            
        except Exception as e:
            logging.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {json_file.name} - {e}")
            return False
    
    async def process_dir_async(self, data_dir: str):
        """
        å¼‚æ­¥æ‰§è¡Œæ‘˜è¦è·å–æµç¨‹
        """
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = list(Path(data_dir).glob("*.json"))
        
        if not json_files:
            logging.error(f"âŒ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶: {data_dir}")
            return
        
        logging.info(f"ğŸ“ å¤„ç†ç›®å½•: {data_dir}: {len(json_files)} ä¸ªæ–‡ä»¶")
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶
        # tmp_confs = ['icassp', 'naacl', 'icaps']
        # tmp_confs = ['icassp', 'icaps']
        # tmp_confs = ['emnlp', 'naacl']
        for json_file in sorted(json_files):
            # if any(tmp_conf in str(json_file.name) for tmp_conf in tmp_confs):
            #     await self.process_file_async(json_file)
            await self.process_file_async(json_file)
        
        # ç»Ÿè®¡å¤„ç†ç»“æœ
        logging.info(f"âœ… æˆåŠŸå¤„ç† {len(json_files)} ä¸ªæ–‡ä»¶")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.print_stats()

    def print_stats(self):
        """
        æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        """
        logging.info("=" * 80)
        logging.info("ğŸ“Š å¼‚æ­¥æ‘˜è¦è·å–ç»Ÿè®¡:")
        
        total_stats = {key: 0 for key in self.stats_keys}
        for file_stats in self.stats_map.values():
            for key in self.stats_keys:
                total_stats[key] += file_stats.get(key, 0)
        
        logging.info(f"ğŸ“„ æ€»è®ºæ–‡æ•°: {total_stats['total_papers']:,}")
        logging.info(f"âœ… å·²æœ‰æ‘˜è¦: {total_stats['papers_with_abstract']:,}")
        logging.info(f"âŒ æ— DOIä¿¡æ¯: {total_stats['papers_without_doi']:,}")
        logging.info(f"ğŸ†• æ–°è·å–æ‘˜è¦: {total_stats['papers_abstract_fetched']:,}")
        logging.info(f"âš ï¸ è·å–å¤±è´¥: {total_stats['papers_abstract_failed']:,}")
        logging.info("=" * 80)

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶ - ä¿®å¤ï¼šå°† Path å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        # stats_for_json = {}
        # for path_obj, stats in self.stats_map.items():
        #     # å°† Path å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä½œä¸ºé”®
        #     stats_for_json[str(path_obj)] = stats

        # with open('stats.json', 'w', encoding='utf-8') as f:
        #     json.dump(stats_for_json, f, ensure_ascii=False, indent=2)

async def main_papers_abstract(data_dir: str, max_concurrent: int = 100, proxy_pool_size: int = 10):
    """
    ä½¿ç”¨å¼‚æ­¥å¤„ç†è·å–è®ºæ–‡æ‘˜è¦çš„ä¸»å‡½æ•°
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    """
    async with AsyncAbstractFetcher(max_concurrent, proxy_pool_size) as fetcher:
        # å¯åŠ¨æµè§ˆå™¨
        await fetcher.driver.start()
        # 1. è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„ä¼šè®®
        await fetcher.process_dir_async(data_dir)
        # å…³é—­æµè§ˆå™¨
        await fetcher.driver.close()