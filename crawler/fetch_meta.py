#!/usr/bin/env python3
"""
DBLP API è®ºæ–‡è·å–æ¨¡å—
é€šè¿‡DBLP APIè·å–æŒ‡å®šä¼šè®®æˆ–æœŸåˆŠçš„è®ºæ–‡ä¿¡æ¯

æ–°å¢å¼‚æ­¥å¤„ç†åŠŸèƒ½ï¼š
1. AsyncAbstractFetcher: å¼‚æ­¥æ‘˜è¦è·å–å™¨
2. æ”¯æŒé«˜å¹¶å‘å¼‚æ­¥è¯·æ±‚
3. æ›´é«˜æ•ˆçš„I/Oå¤„ç†
"""

import os
import re
import sys
import json
import requests
import logging
import time
import asyncio
import aiohttp
import ssl

from typing import Dict, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from typing import Literal


# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(ROOT_DIR)

# å¯¼å…¥CCF Aç±»ä¼šè®®è§„åˆ™
from config.special_rules import get_special_rules
from config.venue import get_venue_name

class DBLPMetaFetcher:
    """DBLPè®ºæ–‡è·å–å™¨"""

    def __init__(self, data_dir: str):
        self.base_url = 'https://dblp.org/search/publ/api'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'DBLP-Paper-Fetcher/1.0'
        })

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.data_dir = data_dir
        if not os.path.exists(data_dir):
            os.makedirs(data_dir, exist_ok=True)

    def _send_request(self, query: str, max_hits: int = 1000, start_from: int = 0) -> Optional[Dict]:
        """
        å‘é€DBLP APIè¯·æ±‚ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            max_hits: æœ€å¤§è¿”å›ç»“æœæ•°
            start_from: èµ·å§‹ä½ç½®

        Returns:
            APIå“åº”çš„JSONæ•°æ®
        """
        params = {
            'q': query,
            'format': 'json',
            'h': min(max_hits, 1000),  # DBLP APIé™åˆ¶æœ€å¤§1000
            'f': start_from,
            'c': 0  # ä¸éœ€è¦è‡ªåŠ¨è¡¥å…¨
        }

        # é‡è¯•é…ç½®ï¼šå»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        retry_delays = [10, 30, 60, 120, 300]  # 10s, 30s, 1min, 2min, 5min
        max_retries = len(retry_delays)

        for attempt in range(max_retries + 1):  # +1 æ˜¯å› ä¸ºç¬¬ä¸€æ¬¡ä¸ç®—é‡è¯•
            try:
                response = self.session.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                if attempt < max_retries:
                    delay = retry_delays[attempt]
                    logging.warning(f"âš ï¸ DBLP APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                    logging.info(f"ğŸ”„ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logging.error(f"âŒ DBLP APIè¯·æ±‚æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {e}")
                    return None

    def _extract_paper_info(self, hit: Dict) -> Dict:
        """
        ä»DBLP APIå“åº”ä¸­æå–è®ºæ–‡ä¿¡æ¯

        Args:
            hit: DBLP APIè¿”å›çš„å•ç¯‡è®ºæ–‡æ•°æ®

        Returns:
            æ ‡å‡†åŒ–çš„è®ºæ–‡ä¿¡æ¯å­—å…¸
        """
        info = hit.get('info', {})

        # æ·»åŠ DBLPç‰¹æœ‰çš„å­—æ®µ
        paper_info = info.copy()
        paper_info['key'] = hit.get('@id', '')  # DBLP key
        paper_info['dblp_url'] = f"https://dblp.org/rec/{hit.get('@id', '')}" if hit.get('@id') else ''

        # ç¡®ä¿eeå­—æ®µæ˜¯åˆ—è¡¨æ ¼å¼
        if 'ee' in paper_info and isinstance(paper_info['ee'], str):
            paper_info['ee'] = [paper_info['ee']]

        return paper_info

    def _get_all_papers_by_page(self, query: str) -> List[Dict]:
        """
        åˆ†é¡µè·å–æ‰€æœ‰è®ºæ–‡æ•°æ®

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            æ‰€æœ‰è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        all_papers = []
        start_from = 0
        page_size = 1000

        while True:
            result = self._send_request(query, max_hits=page_size, start_from=start_from)

            if not result or 'result' not in result:
                break

            hits_data = result['result'].get('hits', {})
            total = int(hits_data.get('@total', 0))
            hits = hits_data.get('hit', [])
            
            if not hits:
                break

            # å¤„ç†å½“å‰é¡µçš„è®ºæ–‡
            for hit in hits:
                paper_info = self._extract_paper_info(hit)
                all_papers.append(paper_info)

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
            if len(all_papers) >= total or len(hits) < page_size:
                break

            start_from += page_size
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

        return all_papers

    def _replace_special_characters(self, text: any) -> str:
        if not text: return ''
        text = str(text)
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰€æœ‰éå­—æ¯å­—ç¬¦ä¸ºç©ºæ ¼
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        # å°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # å»é™¤é¦–å°¾ç©ºæ ¼
        text = text.strip()
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()

        return text

    def fetch_papers(self, venue_name: str, year: int) -> List[Dict]:
        """
        è·å–æŒ‡å®šä¼šè®®/æœŸåˆŠå’Œå¹´ä»½çš„æ‰€æœ‰è®ºæ–‡

        Args:
            venue_name: ä¼šè®®æˆ–æœŸåˆŠåç§° (å¦‚ 'tocs', 'sigmod', 'vldb')
            year: å¹´ä»½

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # 1. è·å–æ­£ç¡®çš„venueåç§°
        dblp_venue_name = get_venue_name(venue_name, year)
        query = f"venue:{dblp_venue_name} year:{year}"

        # 2. è·å–è®ºæ–‡
        papers = self._get_all_papers_by_page(query)

        # 3. è¿‡æ»¤è®ºæ–‡ï¼Œåªä¿ç•™ç²¾ç¡®åŒ¹é…çš„venueï¼ˆDBLP APIæŸ¥è¯¢å’Œè¿”å›ç»“æœçš„venueåç§°æ ¼å¼å¯èƒ½ä¸ä¸€è‡´ï¼‰
        filtered_papers = []

        # ç¡®å®šè¦åŒ¹é…çš„venueåç§°åˆ—è¡¨
        target_venues = [self._replace_special_characters(dblp_venue_name)] # åŸå§‹ venue åç§°
        special_rules = get_special_rules(venue_name) # ç‰¹æ®Šè§„åˆ™ï¼ˆæœ‰äº›åç§°å¤šï¼‰
        if 'filter_venues' in special_rules:
            target_venues.extend([self._replace_special_characters(venue) for venue in special_rules['filter_venues']])

        for paper in papers:
            paper_venue = paper.get('venue', '')
            # å°† paper_venue è½¬æ¢ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨
            if isinstance(paper_venue, list):
                paper_venue = [self._replace_special_characters(venue) for venue in paper_venue]
            else:
                paper_venue = [self._replace_special_characters(paper_venue)]

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•ç›®æ ‡venueåç§°ï¼ˆæŸ¥çœ‹æ˜¯å¦æœ‰äº¤é›†ï¼‰
            if set(paper_venue).intersection(target_venues):
                filtered_papers.append(paper)

        # 4. è¿‡æ»¤å‰ä¿¡æ¯ç»Ÿè®¡
        type_counts_before = {}
        venue_counts_before = {}
        for p in papers:
            # 4.1 ç±»å‹åˆ†å¸ƒ
            paper_type = p.get('type', 'unknown')
            type_counts_before[paper_type] = type_counts_before.get(paper_type, 0) + 1

            # 4.2 æ¥æºåˆ†å¸ƒ
            paper_venue = p.get('venue', 'unknown')
            if isinstance(paper_venue, list):
                paper_venue = ', '.join(paper_venue) if paper_venue else 'unknown'
            elif not isinstance(paper_venue, str):
                paper_venue = str(paper_venue) if paper_venue else 'unknown'
            venue_counts_before[paper_venue] = venue_counts_before.get(paper_venue, 0) + 1

        logging.info(f"âœ… {dblp_venue_name}'{year}: è¿‡æ»¤å‰ {len(papers)} -> è¿‡æ»¤å {len(filtered_papers)}")

        # 5. è¿‡æ»¤åä¿¡æ¯ç»Ÿè®¡
        type_counts = {}
        venue_counts = {}
        for p in filtered_papers:
            # 5.1 ç±»å‹åˆ†å¸ƒ
            paper_type = p.get('type', 'unknown')
            type_counts[paper_type] = type_counts.get(paper_type, 0) + 1

            # 5.2 æ¥æºåˆ†å¸ƒ
            paper_venue = p.get('venue', 'unknown')
            if isinstance(paper_venue, list):
                paper_venue = ', '.join(paper_venue) if paper_venue else 'unknown'
            elif not isinstance(paper_venue, str):
                paper_venue = str(paper_venue) if paper_venue else 'unknown'
            venue_counts[paper_venue] = venue_counts.get(paper_venue, 0) + 1

        return filtered_papers, type_counts, venue_counts, type_counts_before, venue_counts_before

    def check_paper_exists(self, venue_name: str, year: int) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šä¼šè®®/æœŸåˆŠå’Œå¹´ä»½çš„è®ºæ–‡æ•°æ®æ˜¯å¦å·²ç»å­˜åœ¨

        Args:
            venue_name: ä¼šè®®æˆ–æœŸåˆŠåç§°
            year: å¹´ä»½

        Returns:
            å¦‚æœæ•°æ®æ–‡ä»¶å·²å­˜åœ¨åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        clean_venue_name = venue_name.replace(' ', '_').replace('.', '').replace('/', '_')
        filename = f"{clean_venue_name}_{year}.json"
        filepath = os.path.join(self.data_dir, filename)
        return os.path.exists(filepath)

    def save_papers_to_json(self, papers: List[Dict], venue_name: str, year: int,
                           type_counts: Dict, venue_counts: Dict,
                           type_counts_before: Dict, venue_counts_before: Dict,
                           total_papers_before: int) -> str:
        """
        ä¿å­˜è®ºæ–‡æ•°æ®åˆ°JSONæ–‡ä»¶
        Args:
            papers: è¿‡æ»¤åçš„è®ºæ–‡æ•°æ®åˆ—è¡¨
            venue_name: ä¼šè®®/æœŸåˆŠåç§°
            year: å¹´ä»½
            type_counts: è¿‡æ»¤åè®ºæ–‡ç±»å‹åˆ†å¸ƒç»Ÿè®¡
            venue_counts: è¿‡æ»¤åè®ºæ–‡æ¥æºåˆ†å¸ƒç»Ÿè®¡
            type_counts_before: è¿‡æ»¤å‰è®ºæ–‡ç±»å‹åˆ†å¸ƒç»Ÿè®¡
            venue_counts_before: è¿‡æ»¤å‰è®ºæ–‡æ¥æºåˆ†å¸ƒç»Ÿè®¡
            total_papers_before: è¿‡æ»¤å‰è®ºæ–‡æ€»æ•°
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # 1. æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        clean_venue_name = venue_name.replace(' ', '_').replace('.', '').replace('/', '_')
        filename = f"{clean_venue_name}_{year}.json"
        filepath = os.path.join(self.data_dir, filename)

        # 2. æ·»åŠ å…ƒæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
        data_to_save = {
            'metadata': {
                'venue_name': venue_name,
                'year': year,
                'total_papers': len(papers),
                'fetch_time': datetime.now().isoformat(),
                'source': 'DBLP API',
                'type_distribution': type_counts,
                'venue_distribution': venue_counts
            },
            'metadata_before_filtered': {
                'total_papers': total_papers_before,
                'type_distribution': type_counts_before,
                'venue_distribution': venue_counts_before
            },
            'papers': papers
        }

        # 3. ä¿å­˜åˆ°JSONæ–‡ä»¶
        if total_papers_before == 0:
            return None
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)

        return filepath

    def get_papers_by_venue_and_year(self, venue_name: str, year: int) -> str:
        """
        æ ¹æ®ä¼šè®®/æœŸåˆŠåç§°å’Œå¹´ä»½è·å–è®ºæ–‡ï¼Œå¹¶ä¿å­˜ä¸ºJSONæ ¼å¼

        Args:
            venue_name: ä¼šè®®æˆ–æœŸåˆŠåç§°
            year: å¹´ä»½

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # 1. æŸ¥çœ‹è®ºæ–‡æ•°æ®æ˜¯å¦å·²ç»å­˜åœ¨ï¼ˆå·²ç»å­˜åœ¨å¯¹åº”çš„ save çš„æ–‡ä»¶çš„è¯å°±è·³è¿‡å³å¯ï¼‰
        if self.check_paper_exists(venue_name, year):
            logging.info(f"â„¹ï¸ è®ºæ–‡æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡è·å–: {venue_name} {year}")
            return None
        
        # 2. è·å–è®ºæ–‡æ•°æ®
        papers, type_counts, venue_counts, type_counts_before, venue_counts_before = self.fetch_papers(venue_name, year)
        total_papers_before = sum(type_counts_before.values())

        # 3. ä¿å­˜è®ºæ–‡æ•°æ®
        return self.save_papers_to_json(papers, venue_name, year, type_counts, venue_counts,
                                       type_counts_before, venue_counts_before, total_papers_before)

